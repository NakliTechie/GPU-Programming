// Mental Model 10.0

// ... Host code ...

// ===================================
// Device (GPU) Code
// ===================================
__global__ void my_reduction_kernel(...) {
  // 1. Each thread loads one element from global to shared memory.
  __shared__ float tile[...];
  tile[threadIdx.x] = ...;
  __syncthreads();

  // 2. Perform a divergence-free reduction in shared memory.
  //    The number of active threads 's' halves each step.
  for (s = BLOCK_SIZE/2; s > 0; s /= 2) {
    // Active threads are always contiguous: 0..s-1
    if (threadIdx.x < s) {
        // They access data 's' elements away.
        tile[threadIdx.x] += tile[threadIdx.x + s];
    }
    __syncthreads(); // Sync after each level of the tree.
  }

  // 3. Thread 0 of the block writes the block's partial result
  //    back to global memory.
  if (threadIdx.x == 0) {
    global_partial_results[blockIdx.x] = tile[0];
  }
}

// A second kernel launch (or CPU step) is then needed
// to reduce the 'global_partial_results' array.